# -*- coding: utf-8 -*-
"""GNB_LBW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vCh3BvecQrvmHpi3HDUkOwqohG0g8JE1
"""

import matplotlib.pyplot as plt
from matplotlib import style
import numpy as np
style.use('ggplot')
import pandas as pd

#lbw1=pd.read_csv('ML.csv') #,encoding = "ISO-8859-1")

#lbw2=pd.read_csv('ML_1.csv')

# lbw1

#lbw2=lbw2.iloc[:, list(range(9))]
#print(lbw2)

#lbw=lbw1.append(lbw2)

lbw=pd.read_csv('part_1_0.csv')

#lbw



X = lbw.drop("reslt", axis=1)
X=X.drop("history",axis=1)

y = lbw["reslt"]

#y

#X

import pandas as pd
from sklearn import preprocessing
#normalize
x =X #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
X= pd.DataFrame(x_scaled)

X

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)

X_train

y_train

"""Calculated Prior Probabilities for the classes"""

def get_count_unique_vals(labels):
  return dict(labels.value_counts())


def prior_prob(labels):
    counts=get_count_unique_vals(labels)
    number_of_instances=labels.count()
    #counts = labels.value_counts().to_dict()
    print(counts.items())
    priors={(key,value/number_of_instances) for key, value in counts.items()}
    #prior_prob={key,value/}
    return priors
    
priors=prior_prob(y_train)
print(priors)

# yy=y_train==0
# yy.iloc[21]

# filtered_training_set=X_train[(y_train==0)]
# print(filtered_training_set)
# print(y_train[(y_train== 0)])





#y_train

"""Calculate Mean and Variance For all Features"""

import math

def calculate_mean(df):
  return df.mean()
def calculate_std_dev(df):
  return df.std()

def Calculate_Mean_and_Variance(X_train,y_train):
  mean_and_variance_for_class={}
  classes=y_train.unique()
  for everyclass in classes:
      filtered_training_set=X_train[(y_train==everyclass)]
      mean_and_variance=dict()
      for every_attribute in list(X_train.columns.values):
          particular_attribute=filtered_training_set[every_attribute]
          mean_and_variance[every_attribute]=[]
          mean_for_this_attribute=calculate_mean(particular_attribute)
          mean_and_variance[every_attribute].append(mean_for_this_attribute)
          std_dev_for_this_attribute=calculate_std_dev(particular_attribute)
          var_for_this_attribute=math.pow(std_dev_for_this_attribute,2)
          mean_and_variance[every_attribute].append(var_for_this_attribute)
      mean_and_variance_for_class[everyclass]=mean_and_variance
  return mean_and_variance_for_class

dictionary=Calculate_Mean_and_Variance(X_train,y_train)

print((dictionary))



"""NOW using PDF Equation  
Given a feature
"""

def calculate_probability(x, mean, variance):
    exponent = math.exp(-(math.pow(x - mean, 2) / (2 * variance)))
    return (1 / (math.sqrt(2 * math.pi * variance))) * exponent

def predict(X_test,mean_variance):
    predictions = {}
    for _, row in X_test.iterrows():
        #print(_,row)
        results = {}
        for k, v in priors:
            p = 0
            for attr_name in list(X_test.columns.values):
                prob = calculate_probability(row[attr_name], mean_variance[
                    k][attr_name][0], mean_variance[k][attr_name][1])
                if prob > 0:
                    p += math.log(prob)
            results[k] = math.log(v) + p
        predictions[_] = max([key for key in results.keys() if results[
            key] == results[max(results, key=results.get)]])
    return predictions

predictions=predict(X_test,dictionary)

#predictions

#X_test

#y_test

def acc(y_test,prediction):
  count=0
  for ind,row in y_test.iteritems():
    if row == prediction[ind]:
      count+=1
  return count/len(y_test)*100.0

accuracy=acc(y_test,predictions)
print("accuracy",accuracy)

# def calculate_accuracy(test_set, predictions):
#     correct = 0
#     for _, t in test_set.iterrows():
#         if t["reslt"] == predictions[_]:
#             correct += 1
#     return (correct / len(test_set)) * 100.0

#done













# from sklearn.svm import SVC
# svclassifier = SVC(kernel='poly')
# svclassifier.fit(X_train, y_train)

# y_pred = svclassifier.predict(X_test)

# from sklearn.metrics import classification_report, confusion_matrix
# print(confusion_matrix(y_test,y_pred))
# print(classification_report(y_test,y_pred))

# X_train

"""LETS Normalize"""

# import pandas as pd
# from sklearn import preprocessing

# x =X_train.values #returns a numpy array

# min_max_scaler = preprocessing.MinMaxScaler()
# x_scaled = min_max_scaler.fit_transform(x)
# X_train= pd.DataFrame(x_scaled)



y_train

# from sklearn.svm import SVC
# svclassifier = SVC(kernel='rbf',degree=8,tol=1e-7,verbose=True,gamma='scale',shrinking=True)
# svclassifier.fit(X_train, y_train)
# y_pred = svclassifier.predict(X_test)

"""On Train"""

# y_pred_on_train=svclassifier.predict(X_train)

# from sklearn.metrics import classification_report, confusion_matrix
# print(confusion_matrix(y_test,y_pred))
# print(classification_report(y_train,y_pred_on_train))

"""ON Test Set"""

# from sklearn.metrics import classification_report, confusion_matrix
# print(confusion_matrix(y_test,y_pred))
# print(classification_report(y_test,y_pred))

